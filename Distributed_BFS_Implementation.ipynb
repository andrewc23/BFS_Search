{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distributed_BFS_Implementation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2cLeXWqHfG5o5OmM/UO2R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewc23/BFS_Search/blob/main/Distributed_BFS_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4wMSkuPHdrG"
      },
      "source": [
        "##Distributed Implementation of Breadth First Search Using Apache Spark and SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db76OQh_HolK"
      },
      "source": [
        "Function takes in \"G\" (an undirected graph in the form of a Spark Dataframe), \"origins\" (a Python list of dictionaries), and max-depth (a non-negative integer). \n",
        "\n",
        "Function returns a pandas dataframe containing nodes and distances from the origin. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATS1z9G7Hb-j"
      },
      "source": [
        "\n",
        "def spark_bfs(G, origins, max_depth):\n",
        "  \n",
        "  #initialize a view for the review_graph_sdf that was given \n",
        "  G.createOrReplaceTempView('review_graph')\n",
        "\n",
        "  #make the undirected graph\n",
        "  flipped_graph_sdf=spark.sql('''SELECT to_node, from_node FROM review_graph''')\n",
        "  \n",
        "  flipped_graph_sdf.createOrReplaceTempView('flipped_graph')\n",
        "  \n",
        "  #make the undirected graph\n",
        "  undirected_graph_sdf=spark.sql('''SELECT to_node AS node_1, from_node as node_2 FROM flipped_graph UNION ALL SELECT from_node, to_node FROM review_graph ''')\n",
        "\n",
        "  #create view of the undirected_graph_sdf\n",
        "  undirected_graph_sdf.createOrReplaceTempView('undirected_graph')\n",
        "\n",
        "\n",
        "  #create origins spark df\n",
        "  schema_origin = StructType([\n",
        "            StructField(\"node\", StringType(), True)\n",
        "        ])\n",
        "  origins_sdf = spark.createDataFrame(origins, schema_origin)#create the origins sdf \n",
        "  #create the view of the origins sdf\n",
        "  origins_sdf.createOrReplaceTempView('origins_df')\n",
        "\n",
        "\n",
        "  #initiate the frontier variable\n",
        "  frontier_sdf=spark.sql('''SELECT  origins_df.node AS frontier FROM origins_df ''')\n",
        "\n",
        "  #create the view for the frontier\n",
        "  frontier_sdf.createOrReplaceTempView('frontier_df')# only thing in this at this point should be the origins\n",
        "  \n",
        "  #frontier_sdf.show()\n",
        "\n",
        "  #created the visited sdf \n",
        "  visited_sdf=spark.sql('''SELECT frontier_df.frontier as node FROM frontier_df''')\n",
        "\n",
        "  #create the view for the visited sdf\n",
        "  visited_sdf.createOrReplaceTempView('visited_df')\n",
        "\n",
        "  distance = 0\n",
        "  depth =0\n",
        "  #add the distance to the visited \n",
        "  visited_sdf=visited_sdf.withColumn(\"distance\",F.lit(distance))\n",
        "\n",
        "  #create the view for the visited sdf\n",
        "  visited_sdf.createOrReplaceTempView('visited_df')\n",
        "\n",
        "  #visited_sdf.show()\n",
        "\n",
        "  #loop to update the vistied and frontier dfs\n",
        "  while depth < max_depth:\n",
        "\n",
        "    #first update the frontier\n",
        "     frontier_sdf=spark.sql('''SELECT undirected_graph.node_2 as Frontier  FROM frontier_df INNER JOIN undirected_graph ON undirected_graph.node_1=frontier_df.frontier''')\n",
        "\n",
        "     frontier_sdf.createOrReplaceTempView('frontier_df')\n",
        "\n",
        "     #next drop the duplicates and repeats from the frontier\n",
        "\n",
        "     frontier_sdf=frontier_sdf.groupBy('Frontier').count().where('count=1')\n",
        "\n",
        "     frontier_sdf.createOrReplaceTempView('frontier_df')\n",
        "\n",
        "     #update the distance \n",
        "     distance = distance +1\n",
        "\n",
        "     frontier_sdf=frontier_sdf.withColumn(\"distance\",F.lit(distance))\n",
        "\n",
        "     frontier_sdf.createOrReplaceTempView('frontier_df')\n",
        "\n",
        "    #now union the visited with just the origin and the new frontier to get the new visited\n",
        "\n",
        "     visited_sdf=spark.sql('''SELECT frontier_df.Frontier as node, frontier_df.distance FROM frontier_df UNION ALL SELECT visited_df.node, visited_df.distance FROM visited_df ''')\n",
        "     #create the view for the visited sdf\n",
        "     visited_sdf.createOrReplaceTempView('visited_df')\n",
        "\n",
        "\n",
        "    #update the depths and distances\n",
        "     depth=depth+1\n",
        "     #distance=distance+1\n",
        "\n",
        "  #grab the top 75 tuples and conver to pandas dataframe\n",
        "\n",
        "  #find length of the spark df\n",
        "  length_output=visited_sdf.count()\n",
        "\n",
        "  answer_df=spark.sql('''SELECT * FROM  visited_df''')\n",
        "\n",
        "  answer_df=answer_df.toPandas()\n",
        "\n",
        "  answer_75 = answer_df.sort_values(by=\"node\")[:75]\n",
        "\n",
        "  print(length_output)\n",
        "\n",
        "\n",
        "  return (answer_75)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}